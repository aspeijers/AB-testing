---
title: "A/B Testing (aka Hypothesis Testing)"
author: "Anneke Speijers"
date: "11/17/2016"
output: html_document
sansfont: Roboto
documentclass: article
---

A/B testing is a form of statistical hypothesis testing whereby a randomised experiment is carried out to determine whether a difference exists between two variants - a control (A) and a treatment (B).  

Consider the conversion rate (CR) for a particular webpage. Say, a new design for this page is proposed and we need to know whether it will increase the current CR or not. Running an A/B test is often a good way to determine this. 
  
But how many samples do we need to get a statistically significant result? First let's look at how to model the CR statistically.


### Modeling user behaviour using distributions

Each visitor to a page either converts or doesn't. So their behaviour follows a bernoulli distribution with probablity of conversion, p. Let $X_i$ represent the behaviour of visitor i. $X_i=1$ if the visitor converts and $X_i=0$ if the visitor does not convert. 

$$X_i \sim Bernoulli(p)$$

Say we have n visitors to the page. Assuming the behaviour of each visitor is independent and identically distributed, their overall behaviour is just the sum of each individual's behaviour. This, by definition, is a binomial distribution.

$$\sum_{i=1}^n X_i \sim Binomial(n,p)$$

This models the total conversions. To transform this to the CR we simply need to divide by the total number of visitors:

$$Y \sim \frac{1}{n} Binomial(n,p)$$

The mean and variance of Y are then:

$\mu_Y = \frac{np}{n} = p$

$\sigma^2_Y = \frac{np(1-p)}{n^2} = \frac{p(1-p)}{n}$


The Central Limit Theorem tells us that the sum of independently and identically distributed random variables tends to a normal distribution as n goes to infinity. (fix this explanation up!)

$$Z = \frac{Y-\mu_Y}{\sigma_Y} \xrightarrow{d} N(0,1)$$

Or alternatively,
$$Y \xrightarrow{d} N(\mu_Y, \sigma^2_Y) = N(p, \frac{p(1-p)}{n})$$

Let us call the distribution for behaviour on the control page $Y_A$ and the distribution for behaviour on the treatment page $Y_B$. 

$$Y_A \sim N(p_A, \frac{p_A(1-p_A)}{n_A})$$

$$Y_B \sim N(p_B, \frac{p_B(1-p_B)}{n_B})$$

We can then look at the difference between these two distributions. Remember! - the difference between two normally distributed random variables, is a normally distributed random variable. 

$$Y_A - Y_B \sim N(p_B-p_A, \frac{p_A(1-p_A)}{n_A} + \frac{p_B(1-p_B)}{n_B})$$


### One sided test - null and alternative hypotheses

The null hypothesis $(H_0)$ in this case, is that no difference exists between the existing control and the treatment. That is that $p_A=p_B$. 

$$Y_{H_0} \sim N(0, 2\frac{p_A(1-p_A)}{n_A})$$

The alternative hypothesis $H_{Alt}$, is that a difference exists. Note that we assume $n_A = n_B$. 

$$Y_{H_{Alt}} \sim N(p_B-p_A, \frac{p_A(1-p_A) + p_B(1-p_B)}{n_A})$$

#### Errors
There are two types of errors that can be encountered during hypothesis testing; type I and type II.

**Type I**: $H_0$ is rejected when in fact it is true. That is, there appears to be a difference in the conversion rates, when in fact no difference exists. This error is characterised by the **significance**, $\alpha$, of the test. Typical values for $\alpha$ are 0.05, 0.01 and 0.001. $(1-\alpha)$ is refered to as the **reliability**. 

**Type II**: $H_0$ is not rejected when in fact it is not true. That is, there appears to be no difference in the conversion rates, when in fact a difference exists. This error is characterised by the **power**, $(1-\beta)$, of the test, which reflects the opposite of this, ie the probability of rejecting the $H_0$ when it is not true. A typical value for $\beta$ is 0.2. 


```{r}

```


```{r, echo=FALSE}

```

